%-------------------------------------------------------------------------------
 %	REPLACE WITH YOUR PUBLICATIONS
 %-------------------------------------------------------------------------------
 @InProceedings{ChenfeiICML,
  title = 	  {Deep Neural Network Fusion via Graph Matching with Applications to Model Ensemble and Federated Learning},
  author =    {Liu, Chang and Lou, Chenfei and Wang, Runzhong and Xi, Alan Yuhan and Shen, Li and Yan, Junchi},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	  {13857--13869},
  year = 	    {2022},
  editor = 	  {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	  {162},
  series = 	  {Proceedings of Machine Learning Research},
  month = 	  {17--23 Jul},
  publisher = {PMLR},
  pdf = 	    {https://proceedings.mlr.press/v162/liu22k/liu22k.pdf},
  url = 	    {https://proceedings.mlr.press/v162/liu22k.html},
  abstract = 	{Model fusion without accessing training data in machine learning has attracted increasing interest due to 
               the practical resource-saving and data privacy issues. During the training process, the neural weights of 
               each model can be randomly permuted, and we have to align the channels of each layer before fusing them. 
               Regrading the channels as nodes and weights as edges, aligning the channels to maximize weight similarity 
               is a challenging NP-hard assignment problem. Due to its quadratic assignment nature, we formulate the model 
               fusion problem as a graph matching task, considering the second-order similarity of model weights instead of 
               previous work merely formulating model fusion as a linear assignment problem. For the rising problem scale 
               and multi-model consistency issues, we propose an efficient graduated assignment-based model fusion method, 
               dubbed GAMF, which iteratively updates the matchings in a consistency-maintaining manner. We apply GAMF to 
               tackle the compact model ensemble task and federated learning task on MNIST, CIFAR-10, CIFAR-100, and 
               Tiny-Imagenet. The performance shows the efficacy of our GAMF compared to state-of-the-art baselines.}
 }
 
 @misc{ChenfeiAAAI,
  title={Predictive Exit: Prediction of Fine-Grained Early Exits for Computation- and Energy-Efficient Inference}, 
  author={Xiangjie Li and Chenfei Lou and Zhengping Zhu and Yuchi Chen and Yingtao Shen and Yehan Ma and An Zou},
  year={2022},
  eprint={2206.04685},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
 }

 @INPROCEEDINGS{ChenfeiISCAS,
  author={Lou, Chenfei and Xiao, Weihua and Qian, Weikang},
  booktitle={2022 IEEE International Symposium on Circuits and Systems (ISCAS)}, 
  title={Quantified Satisfiability-based Simultaneous Selection of Multiple Local Approximate Changes under Maximum Error Bound}, 
  year={2022},
  volume={},
  number={},
  pages={3498-3502},
  doi={10.1109/ISCAS48785.2022.9937529}
  }
